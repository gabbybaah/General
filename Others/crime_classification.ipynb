{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the LAPD's crime classifications\n",
    "\r\n",
    "Examining the categorization of crimes by the LAPD, The Times scrutinized violent crime data spanning from 2005 to 2012. The analysis revealed a discrepancy, indicating that approximately 14,000 severe assaults were mistakenly classified as minor offenses, resulting in an artificial reduction of the city's reported crime rates. The investigative process involved the application of an algorithm that employed two machine learning classifiers. These classifiers assessed concise crime descriptions to distinguish between minor and serious assaults, unveiling the extent of misclassifications within the LAPD's crime data\n",
    "\n",
    "This project was sourced from a Github repository to demonstrate the application of Machine Learning in different areas.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk.classify import MaxentClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming and stop words\n",
    "\n",
    "We're going to clean up the crime descriptions in two steps. First, we're going to [stem](https://en.wikipedia.org/wiki/Stemming) the words -- this reduces the words to their root in order to limit differences based on tense or whether they appear in the plural or possessive form. Then, we're going to strip out a custom list of [stop words](https://en.wikipedia.org/wiki/Stop_words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Define a standard snowball stemmer\n",
    "STEMMER = SnowballStemmer('english')\n",
    "# Make a list of stopwords, including the stemmed versions\n",
    "# These are words that have no impact on the classification, and\n",
    "# can even occasionally mess up the classifier.\n",
    "STOPWORDS = [\n",
    "    'susp',\n",
    "    'susps',\n",
    "    's',\n",
    "    'v',\n",
    "    'in',\n",
    "    'ppa',\n",
    "    'vict',\n",
    "    'the',\n",
    "    'and',\n",
    "    '&',\n",
    "    '-s',\n",
    "    'after',\n",
    "    'for',\n",
    "    'heard',\n",
    "    'second',\n",
    "    'avoid',\n",
    "    'hold',\n",
    "    'holding',\n",
    "    'retrieved',\n",
    "    'battery',\n",
    "    'fist',\n",
    "    'of',\n",
    "    'to',\n",
    "    'a',\n",
    "]\n",
    "STOPWORDS += [STEMMER.stem(i) for i in STOPWORDS]\n",
    "STOPWORDS = list(set(STOPWORDS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize\n",
    "\n",
    "This is a function to take a description and break it up into the individual \"features\" we're going to use to classify it. We separate the description into individual words, then stem them and remove stop words. From there, we make a list of individual words and then combine them into [bigrams](https://en.wikipedia.org/wiki/Bigram)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def tokenize(description):\n",
    "    \"\"\"\n",
    "    Takes LAPD description text, strips out unwanted words and text,\n",
    "    and prepares it for the trainer.\n",
    "    \"\"\"\n",
    "    # first lower case and strip leading/trailing whitespace\n",
    "    description = description.lower().strip()\n",
    "    # kill the 'do-'s and any stray punctuation\n",
    "    description = description.replace('do-', '').replace('.', '').replace(',', '')\n",
    "    # make a list of words by splitting on whitespace\n",
    "    words = description.split(' ')\n",
    "    # Make sure each \"word\" is a real string / account for odd whitespace\n",
    "    words = [STEMMER.stem(i) for i in words if i]\n",
    "    words = [i for i in words if i not in STOPWORDS]\n",
    "    # let's see if adding bigrams improves the accuracy\n",
    "    bigrams = ngrams(words, 2)\n",
    "    bigrams = [\"%s|%s\" % (i[0], i[1]) for i in bigrams]\n",
    "    # bigrams = [i for i in bigrams if STEMMED_BIGRAMS.get(i)]\n",
    "    # set up a dict\n",
    "    out_dict = dict([(i, True) for i in words + bigrams])\n",
    "    # The NLTK trainer expects data in a certain format\n",
    "    return out_dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grab the features\n",
    "\n",
    "Loop through our example CSV and grab the features we're going to use to train our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# open our sample file and use the CSV module to parse it\n",
    "f = open('training_data.csv', 'rU')\n",
    "data = list(csv.DictReader(f))\n",
    "# Make an empty list for our processed data\n",
    "features = []\n",
    "# Loop through all the lines in the CSV\n",
    "for i in data:\n",
    "    description = i.get('NARRATIVE')\n",
    "    classification = i.get('classification')\n",
    "    feats = tokenize(description)\n",
    "    features.append((feats, classification))\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({u'kick|polic': True, u'use': True, u'his': True, u'leg': True, u'polic': True, u'under|arrest': True, u'right|leg': True, u'place|under': True, u'back': True, u'sergeant|back': True, u'arrest': True, u'right': True, u'place': True, u'sergeant': True, u'use|his': True, u'under': True, u'his|right': True, u'arrest|use': True, u'leg|kick': True, u'kick': True, u'polic|sergeant': True}, 'minor')\n"
     ]
    }
   ],
   "source": [
    "# Here's what this looks like\n",
    "print features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the classifiers\n",
    "\n",
    "For this analysis we used two machine learning classifiers. The first is a linear [support vector machine](http://nlp.stanford.edu/IR-book/html/htmledition/support-vector-machines-the-linearly-separable-case-1.html) from the stellar [scikit-learn Python library](http://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html). The second is a [maximum entropy classifier](http://www.nltk.org/book/ch06.html#maximum-entropy-classifiers). For the official analysis I used the [MegaM](http://www.umiacs.umd.edu/~hal/megam/) optimization package to dramatically improve the training speed. Here, for simplicity, I'm using the NLTK built in trainer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<SklearnClassifier(Pipeline(steps=[('tfidf', TfidfTransformer(norm=u'l2', smooth_idf=True, sublinear_tf=False,\n",
       "         use_idf=True)), ('linearsvc', LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0))]))>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train our classifiers. Let's start with Linear SVC\n",
    "# Make a data prep pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('linearsvc', LinearSVC()),\n",
    "])\n",
    "# make the classifier\n",
    "linear_svc = SklearnClassifier(pipeline)\n",
    "# Train it\n",
    "linear_svc.train(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ==> Training (100 iterations)\n",
      "\n",
      "      Iteration    Log Likelihood    Accuracy\n",
      "      ---------------------------------------\n",
      "             1          -0.69315        0.500\n",
      "             2          -0.43483        0.970\n",
      "             3          -0.32266        0.990\n",
      "             4          -0.25840        0.990\n",
      "             5          -0.21623        1.000\n",
      "             6          -0.18624        1.000\n",
      "             7          -0.16375        1.000\n",
      "             8          -0.14621        1.000\n",
      "             9          -0.13215        1.000\n",
      "            10          -0.12061        1.000\n",
      "            11          -0.11097        1.000\n",
      "            12          -0.10279        1.000\n",
      "            13          -0.09577        1.000\n",
      "            14          -0.08967        1.000\n",
      "            15          -0.08432        1.000\n",
      "            16          -0.07959        1.000\n",
      "            17          -0.07538        1.000\n",
      "            18          -0.07161        1.000\n",
      "            19          -0.06821        1.000\n",
      "            20          -0.06513        1.000\n",
      "            21          -0.06232        1.000\n",
      "            22          -0.05976        1.000\n",
      "            23          -0.05741        1.000\n",
      "            24          -0.05524        1.000\n",
      "            25          -0.05324        1.000\n",
      "            26          -0.05139        1.000\n",
      "            27          -0.04966        1.000\n",
      "            28          -0.04805        1.000\n",
      "            29          -0.04655        1.000\n",
      "            30          -0.04514        1.000\n",
      "            31          -0.04383        1.000\n",
      "            32          -0.04258        1.000\n",
      "            33          -0.04142        1.000\n",
      "            34          -0.04031        1.000\n",
      "            35          -0.03927        1.000\n",
      "            36          -0.03828        1.000\n",
      "            37          -0.03735        1.000\n",
      "            38          -0.03646        1.000\n",
      "            39          -0.03561        1.000\n",
      "            40          -0.03481        1.000\n",
      "            41          -0.03404        1.000\n",
      "            42          -0.03331        1.000\n",
      "            43          -0.03261        1.000\n",
      "            44          -0.03194        1.000\n",
      "            45          -0.03130        1.000\n",
      "            46          -0.03069        1.000\n",
      "            47          -0.03010        1.000\n",
      "            48          -0.02953        1.000\n",
      "            49          -0.02899        1.000\n",
      "            50          -0.02847        1.000\n",
      "            51          -0.02797        1.000\n",
      "            52          -0.02748        1.000\n",
      "            53          -0.02702        1.000\n",
      "            54          -0.02657        1.000\n",
      "            55          -0.02613        1.000\n",
      "            56          -0.02571        1.000\n",
      "            57          -0.02531        1.000\n",
      "            58          -0.02492        1.000\n",
      "            59          -0.02454        1.000\n",
      "            60          -0.02417        1.000\n",
      "            61          -0.02382        1.000\n",
      "            62          -0.02347        1.000\n",
      "            63          -0.02314        1.000\n",
      "            64          -0.02281        1.000\n",
      "            65          -0.02250        1.000\n",
      "            66          -0.02219        1.000\n",
      "            67          -0.02190        1.000\n",
      "            68          -0.02161        1.000\n",
      "            69          -0.02133        1.000\n",
      "            70          -0.02106        1.000\n",
      "            71          -0.02079        1.000\n",
      "            72          -0.02053        1.000\n",
      "            73          -0.02028        1.000\n",
      "            74          -0.02004        1.000\n",
      "            75          -0.01980        1.000\n",
      "            76          -0.01957        1.000\n",
      "            77          -0.01934        1.000\n",
      "            78          -0.01912        1.000\n",
      "            79          -0.01890        1.000\n",
      "            80          -0.01869        1.000\n",
      "            81          -0.01849        1.000\n",
      "            82          -0.01828        1.000\n",
      "            83          -0.01809        1.000\n",
      "            84          -0.01790        1.000\n",
      "            85          -0.01771        1.000\n",
      "            86          -0.01753        1.000\n",
      "            87          -0.01735        1.000\n",
      "            88          -0.01717        1.000\n",
      "            89          -0.01700        1.000\n",
      "            90          -0.01683        1.000\n",
      "            91          -0.01667        1.000\n",
      "            92          -0.01650        1.000\n",
      "            93          -0.01635        1.000\n",
      "            94          -0.01619        1.000\n",
      "            95          -0.01604        1.000\n",
      "            96          -0.01589        1.000\n",
      "            97          -0.01575        1.000\n",
      "            98          -0.01560        1.000\n",
      "            99          -0.01546        1.000\n",
      "         Final          -0.01532        1.000\n"
     ]
    }
   ],
   "source": [
    "# Next, let's do the Maximum Entropy\n",
    "maxent = MaxentClassifier.train(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the classifiers\n",
    "\n",
    "Now let's test these out! For this example we're only using a training sample of 100 crimes, which is not going to produce very accurate results. For our official analysis, we used a training sample of more than 20,000 crimes we reviewed as part of a previous story in 2014. We also chose to use two classifiers because, though they agreed on the vast majority of crimes, each classifier did a better job with some edge cases we didn't want to miss. You can check out the results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "correct: minor | maxent: serious | linear svc: serious\n",
      "correct: minor | maxent: minor | linear svc: minor\n",
      "correct: minor | maxent: serious | linear svc: serious\n",
      "correct: serious | maxent: serious | linear svc: serious\n",
      "correct: serious | maxent: serious | linear svc: serious\n",
      "correct: minor | maxent: minor | linear svc: minor\n",
      "correct: minor | maxent: minor | linear svc: minor\n",
      "correct: minor | maxent: serious | linear svc: serious\n",
      "correct: serious | maxent: serious | linear svc: serious\n",
      "correct: minor | maxent: minor | linear svc: minor\n",
      "correct: minor | maxent: minor | linear svc: minor\n"
     ]
    }
   ],
   "source": [
    "# Now, let's try these out\n",
    "test_data = list(csv.DictReader(open('test_data.csv', 'rU')))\n",
    "for i in test_data:\n",
    "    description = i.get('NARRATIVE')\n",
    "    classification = i.get('classification')\n",
    "    toks = tokenize(description)\n",
    "    # now grab the results of our classifiers\n",
    "    maxent_class = maxent.classify(toks)\n",
    "    svc_class = linear_svc.classify(toks)\n",
    "    print('correct: %s | maxent: %s | linear svc: %s' % (classification, maxent_class, svc_class))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
